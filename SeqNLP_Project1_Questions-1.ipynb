{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"SeqNLP_Project1_Questions-1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xT7MKZuMRaCg"},"source":["# Sentiment Classification\n","\n","\n","### Generate Word Embeddings and retrieve outputs of each layer with Keras based on Classification task\n","\n","Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n","\n","It is a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n","\n","We willl use the imdb dataset to learn word embeddings as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with sentiment (positive or negative). \n","\n","\n","\n","### Dataset\n","\n","`from keras.datasets import imdb`\n","\n","Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n","\n","As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n","\n","\n","### Aim\n","\n","1. Import test and train data  \n","2. Import the labels ( train and test) \n","3. Get the word index and then Create key value pair for word and word_id. (12.5 points)\n","4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)\n","5. Report the Accuracy of the model. (5 points)  \n","6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wq4RCyyPSYRp"},"source":["#### Usage:"]},{"cell_type":"markdown","metadata":{"id":"NAuHJoJYFVoM","colab_type":"text"},"source":["The numpy version is changed from 1.16.4 to 1.16.1 which has allow_pickel=True *Source: Google"]},{"cell_type":"code","metadata":{"id":"0JbNBQQkZH-l","colab_type":"code","colab":{}},"source":["import numpy as np\n","# save np.load\n","np_load_old = np.load\n","\n","# modify the default parameters of np.load\n","np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"NGCtiXUhSWss","colab":{}},"source":["from keras.datasets import imdb\n","vocab_size = 10000 #vocab size\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xpyvyq1N_Y9L","colab_type":"code","colab":{}},"source":["from keras.preprocessing.sequence import pad_sequences\n","vocab_size = 10000 #vocab size\n","maxlen = 300  #number of word used from each review"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tous0JOAJELL","colab_type":"text"},"source":["What i didn't understand is, why we executed the previous line of code to import dataset when we are doing the same below!!"]},{"cell_type":"code","metadata":{"id":"ewFA3hSE_Y9N","colab_type":"code","colab":{}},"source":["#load dataset as a list of ints\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n","#make all sequences of the same length\n","x_train = pad_sequences(x_train, maxlen=maxlen)\n","x_test =  pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Ut5nh-LZTWj","colab_type":"code","colab":{}},"source":["# restore np.load for future normal usage\n","np.load = np_load_old"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBzs4C1U_Y9Q","colab_type":"code","outputId":"d80731fe-dfc2-4045-d336-359dfede4d83","executionInfo":{"status":"ok","timestamp":1566629762650,"user_tz":-330,"elapsed":1038,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["x_train.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000, 300)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"I5DAUOB2GH2t","colab_type":"code","outputId":"12a898ee-5c65-4ec1-964c-f01987ec11cc","executionInfo":{"status":"ok","timestamp":1566629763976,"user_tz":-330,"elapsed":736,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["import collections\n","counter = collections.Counter(y_train)\n","print (counter)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Counter({1: 12500, 0: 12500})\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nLgLhXlg_Y9S","colab_type":"code","outputId":"b4b3e5c8-4569-4f66-dd33-745206442811","executionInfo":{"status":"ok","timestamp":1566629765323,"user_tz":-330,"elapsed":692,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["y_train.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25000,)"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"gwLp9xvmIdhd","colab_type":"text"},"source":["So we have x_train with 25000 rows and each row is populated with 300 words/cols.\n","And there are 12500 positive reviews and 12500 negative reviews in the given dataset."]},{"cell_type":"markdown","metadata":{"id":"cCwTunGS_Y9o","colab_type":"text"},"source":["## Build Keras Embedding Layer Model\n","We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n","\n","* The embedding layer can be used at the start of a larger deep learning model. \n","* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n","* Use the embedding layer to train our own word2vec models.\n","\n","The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."]},{"cell_type":"code","metadata":{"id":"U68VjkuF_Y9p","colab_type":"code","colab":{}},"source":["from keras.preprocessing import sequence\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.layers.embeddings import Embedding"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BaOtCQdM_Y9u","colab_type":"code","colab":{}},"source":["model = Sequential()\n","model.add(Embedding(vocab_size, 32, input_length=maxlen))\n","model.add(Flatten())\n","model.add(Dense(300, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-5vOZN3d_Y9x","colab_type":"code","outputId":"0e61abe0-94f6-4251-f180-ac36690598b1","executionInfo":{"status":"ok","timestamp":1566629802294,"user_tz":-330,"elapsed":29455,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":745}},"source":["model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=20, batch_size=128, verbose=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/20\n"," - 2s - loss: 0.4544 - acc: 0.7620 - val_loss: 0.2972 - val_acc: 0.8709\n","Epoch 2/20\n"," - 1s - loss: 0.1359 - acc: 0.9521 - val_loss: 0.3360 - val_acc: 0.8632\n","Epoch 3/20\n"," - 1s - loss: 0.0237 - acc: 0.9948 - val_loss: 0.4483 - val_acc: 0.8618\n","Epoch 4/20\n"," - 1s - loss: 0.0027 - acc: 0.9999 - val_loss: 0.4967 - val_acc: 0.8642\n","Epoch 5/20\n"," - 1s - loss: 8.6831e-04 - acc: 1.0000 - val_loss: 0.5267 - val_acc: 0.8654\n","Epoch 6/20\n"," - 1s - loss: 4.7719e-04 - acc: 1.0000 - val_loss: 0.5491 - val_acc: 0.8658\n","Epoch 7/20\n"," - 1s - loss: 3.0908e-04 - acc: 1.0000 - val_loss: 0.5675 - val_acc: 0.8662\n","Epoch 8/20\n"," - 1s - loss: 2.1864e-04 - acc: 1.0000 - val_loss: 0.5833 - val_acc: 0.8664\n","Epoch 9/20\n"," - 1s - loss: 1.6182e-04 - acc: 1.0000 - val_loss: 0.5975 - val_acc: 0.8668\n","Epoch 10/20\n"," - 1s - loss: 1.2355e-04 - acc: 1.0000 - val_loss: 0.6105 - val_acc: 0.8670\n","Epoch 11/20\n"," - 1s - loss: 9.7151e-05 - acc: 1.0000 - val_loss: 0.6225 - val_acc: 0.8672\n","Epoch 12/20\n"," - 1s - loss: 7.7246e-05 - acc: 1.0000 - val_loss: 0.6339 - val_acc: 0.8669\n","Epoch 13/20\n"," - 1s - loss: 6.2440e-05 - acc: 1.0000 - val_loss: 0.6437 - val_acc: 0.8675\n","Epoch 14/20\n"," - 1s - loss: 5.1094e-05 - acc: 1.0000 - val_loss: 0.6536 - val_acc: 0.8670\n","Epoch 15/20\n"," - 1s - loss: 4.2177e-05 - acc: 1.0000 - val_loss: 0.6635 - val_acc: 0.8672\n","Epoch 16/20\n"," - 1s - loss: 3.5218e-05 - acc: 1.0000 - val_loss: 0.6721 - val_acc: 0.8672\n","Epoch 17/20\n"," - 1s - loss: 2.9342e-05 - acc: 1.0000 - val_loss: 0.6810 - val_acc: 0.8670\n","Epoch 18/20\n"," - 1s - loss: 2.4772e-05 - acc: 1.0000 - val_loss: 0.6895 - val_acc: 0.8672\n","Epoch 19/20\n"," - 1s - loss: 2.0980e-05 - acc: 1.0000 - val_loss: 0.6976 - val_acc: 0.8672\n","Epoch 20/20\n"," - 1s - loss: 1.7856e-05 - acc: 1.0000 - val_loss: 0.7054 - val_acc: 0.8674\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f309af345f8>"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"EpJ5jR3xMaYq","colab_type":"text"},"source":["We are getting accuracy of higher 86%.\n","This means the model is able to predict 87% of sentiments/reviews correctly as positive or negative"]},{"cell_type":"code","metadata":{"id":"58OrFZyPMHOp","colab_type":"code","outputId":"ad97b15d-0de1-4db4-9ec3-5f6b9d90c973","executionInfo":{"status":"ok","timestamp":1566629808772,"user_tz":-330,"elapsed":2023,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["scores = model.evaluate(x_test, y_test, verbose=2)\n","print(scores)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0.7054031683260202, 0.86736]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2XOBLnxP_Y9z","colab_type":"text"},"source":["## Retrive the output of each layer in keras for a given single test sample from the trained model you built"]},{"cell_type":"markdown","metadata":{"id":"N8QgW9S0NXP1","colab_type":"text"},"source":["\"I am not able to understand the question and its purpose itself\""]},{"cell_type":"code","metadata":{"id":"cmrEpu7h_Y91","colab_type":"code","colab":{}},"source":["##**I am herewith trying to improve the accuracy (if possible) by playing with the vocab size**##"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DnXmAyL1NpIs","colab_type":"code","colab":{}},"source":["vocab_size1 = 100000 #vocab size increased 10 times\n","#maxlen1 = 300 # each row will now have 3000 words, thereby increased it 10 times as well"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rs6SzvNXODTx","colab_type":"code","colab":{}},"source":["#load dataset as a list of ints\n","(xtrain, ytrain), (xtest, ytest) = imdb.load_data(num_words=vocab_size1)\n","#make all sequences of the same length\n","xtrain = pad_sequences(xtrain, maxlen=maxlen)\n","xtest =  pad_sequences(xtest, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CViSPYFgOUFB","colab_type":"code","colab":{}},"source":["model1 = Sequential()\n","model1.add(Embedding(vocab_size1, 32, input_length=maxlen))\n","model1.add(Flatten())\n","model1.add(Dense(300, activation='relu'))\n","model1.add(Dense(1, activation='sigmoid'))\n","model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsoSq_jQOjHN","colab_type":"code","outputId":"c4cbb0f9-ec76-4b7e-8995-cd71fdba4a3b","executionInfo":{"status":"ok","timestamp":1566630005028,"user_tz":-330,"elapsed":176626,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model1.fit(xtrain, ytrain, validation_data=(xtest, ytest), epochs=100, batch_size=128, verbose=2)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 25000 samples, validate on 25000 samples\n","Epoch 1/100\n"," - 2s - loss: 0.4442 - acc: 0.7660 - val_loss: 0.2969 - val_acc: 0.8721\n","Epoch 2/100\n"," - 2s - loss: 0.0986 - acc: 0.9662 - val_loss: 0.3418 - val_acc: 0.8650\n","Epoch 3/100\n"," - 2s - loss: 0.0106 - acc: 0.9986 - val_loss: 0.4057 - val_acc: 0.8664\n","Epoch 4/100\n"," - 2s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.4378 - val_acc: 0.8672\n","Epoch 5/100\n"," - 2s - loss: 5.6906e-04 - acc: 1.0000 - val_loss: 0.4556 - val_acc: 0.8678\n","Epoch 6/100\n"," - 2s - loss: 3.4376e-04 - acc: 1.0000 - val_loss: 0.4713 - val_acc: 0.8688\n","Epoch 7/100\n"," - 2s - loss: 2.2961e-04 - acc: 1.0000 - val_loss: 0.4849 - val_acc: 0.8684\n","Epoch 8/100\n"," - 2s - loss: 1.6433e-04 - acc: 1.0000 - val_loss: 0.4969 - val_acc: 0.8690\n","Epoch 9/100\n"," - 2s - loss: 1.2304e-04 - acc: 1.0000 - val_loss: 0.5079 - val_acc: 0.8688\n","Epoch 10/100\n"," - 2s - loss: 9.4843e-05 - acc: 1.0000 - val_loss: 0.5177 - val_acc: 0.8691\n","Epoch 11/100\n"," - 2s - loss: 7.4819e-05 - acc: 1.0000 - val_loss: 0.5266 - val_acc: 0.8694\n","Epoch 12/100\n"," - 2s - loss: 5.9954e-05 - acc: 1.0000 - val_loss: 0.5353 - val_acc: 0.8697\n","Epoch 13/100\n"," - 2s - loss: 4.8713e-05 - acc: 1.0000 - val_loss: 0.5435 - val_acc: 0.8699\n","Epoch 14/100\n"," - 2s - loss: 4.0012e-05 - acc: 1.0000 - val_loss: 0.5511 - val_acc: 0.8699\n","Epoch 15/100\n"," - 2s - loss: 3.3152e-05 - acc: 1.0000 - val_loss: 0.5585 - val_acc: 0.8702\n","Epoch 16/100\n"," - 2s - loss: 2.7827e-05 - acc: 1.0000 - val_loss: 0.5657 - val_acc: 0.8704\n","Epoch 17/100\n"," - 2s - loss: 2.3342e-05 - acc: 1.0000 - val_loss: 0.5725 - val_acc: 0.8705\n","Epoch 18/100\n"," - 2s - loss: 1.9779e-05 - acc: 1.0000 - val_loss: 0.5790 - val_acc: 0.8706\n","Epoch 19/100\n"," - 2s - loss: 1.6866e-05 - acc: 1.0000 - val_loss: 0.5853 - val_acc: 0.8708\n","Epoch 20/100\n"," - 2s - loss: 1.4405e-05 - acc: 1.0000 - val_loss: 0.5919 - val_acc: 0.8706\n","Epoch 21/100\n"," - 2s - loss: 1.2388e-05 - acc: 1.0000 - val_loss: 0.5976 - val_acc: 0.8708\n","Epoch 22/100\n"," - 2s - loss: 1.0663e-05 - acc: 1.0000 - val_loss: 0.6036 - val_acc: 0.8710\n","Epoch 23/100\n"," - 2s - loss: 9.2361e-06 - acc: 1.0000 - val_loss: 0.6094 - val_acc: 0.8709\n","Epoch 24/100\n"," - 2s - loss: 7.9880e-06 - acc: 1.0000 - val_loss: 0.6150 - val_acc: 0.8709\n","Epoch 25/100\n"," - 2s - loss: 6.9698e-06 - acc: 1.0000 - val_loss: 0.6207 - val_acc: 0.8708\n","Epoch 26/100\n"," - 2s - loss: 6.0781e-06 - acc: 1.0000 - val_loss: 0.6261 - val_acc: 0.8709\n","Epoch 27/100\n"," - 2s - loss: 5.3101e-06 - acc: 1.0000 - val_loss: 0.6313 - val_acc: 0.8710\n","Epoch 28/100\n"," - 2s - loss: 4.6539e-06 - acc: 1.0000 - val_loss: 0.6365 - val_acc: 0.8708\n","Epoch 29/100\n"," - 2s - loss: 4.0920e-06 - acc: 1.0000 - val_loss: 0.6415 - val_acc: 0.8708\n","Epoch 30/100\n"," - 2s - loss: 3.6034e-06 - acc: 1.0000 - val_loss: 0.6465 - val_acc: 0.8709\n","Epoch 31/100\n"," - 2s - loss: 3.1772e-06 - acc: 1.0000 - val_loss: 0.6517 - val_acc: 0.8712\n","Epoch 32/100\n"," - 2s - loss: 2.8189e-06 - acc: 1.0000 - val_loss: 0.6563 - val_acc: 0.8710\n","Epoch 33/100\n"," - 2s - loss: 2.4959e-06 - acc: 1.0000 - val_loss: 0.6610 - val_acc: 0.8710\n","Epoch 34/100\n"," - 2s - loss: 2.2129e-06 - acc: 1.0000 - val_loss: 0.6657 - val_acc: 0.8709\n","Epoch 35/100\n"," - 2s - loss: 1.9679e-06 - acc: 1.0000 - val_loss: 0.6704 - val_acc: 0.8712\n","Epoch 36/100\n"," - 2s - loss: 1.7563e-06 - acc: 1.0000 - val_loss: 0.6749 - val_acc: 0.8710\n","Epoch 37/100\n"," - 2s - loss: 1.5685e-06 - acc: 1.0000 - val_loss: 0.6792 - val_acc: 0.8713\n","Epoch 38/100\n"," - 2s - loss: 1.4040e-06 - acc: 1.0000 - val_loss: 0.6837 - val_acc: 0.8711\n","Epoch 39/100\n"," - 2s - loss: 1.2594e-06 - acc: 1.0000 - val_loss: 0.6881 - val_acc: 0.8710\n","Epoch 40/100\n"," - 2s - loss: 1.1291e-06 - acc: 1.0000 - val_loss: 0.6923 - val_acc: 0.8709\n","Epoch 41/100\n"," - 2s - loss: 1.0151e-06 - acc: 1.0000 - val_loss: 0.6967 - val_acc: 0.8710\n","Epoch 42/100\n"," - 2s - loss: 9.1602e-07 - acc: 1.0000 - val_loss: 0.7008 - val_acc: 0.8710\n","Epoch 43/100\n"," - 2s - loss: 8.2711e-07 - acc: 1.0000 - val_loss: 0.7048 - val_acc: 0.8710\n","Epoch 44/100\n"," - 2s - loss: 7.4810e-07 - acc: 1.0000 - val_loss: 0.7090 - val_acc: 0.8710\n","Epoch 45/100\n"," - 2s - loss: 6.7754e-07 - acc: 1.0000 - val_loss: 0.7131 - val_acc: 0.8707\n","Epoch 46/100\n"," - 2s - loss: 6.1672e-07 - acc: 1.0000 - val_loss: 0.7169 - val_acc: 0.8712\n","Epoch 47/100\n"," - 2s - loss: 5.6119e-07 - acc: 1.0000 - val_loss: 0.7209 - val_acc: 0.8708\n","Epoch 48/100\n"," - 2s - loss: 5.1227e-07 - acc: 1.0000 - val_loss: 0.7247 - val_acc: 0.8709\n","Epoch 49/100\n"," - 2s - loss: 4.6829e-07 - acc: 1.0000 - val_loss: 0.7285 - val_acc: 0.8710\n","Epoch 50/100\n"," - 2s - loss: 4.2998e-07 - acc: 1.0000 - val_loss: 0.7322 - val_acc: 0.8710\n","Epoch 51/100\n"," - 2s - loss: 3.9493e-07 - acc: 1.0000 - val_loss: 0.7360 - val_acc: 0.8709\n","Epoch 52/100\n"," - 2s - loss: 3.6395e-07 - acc: 1.0000 - val_loss: 0.7396 - val_acc: 0.8708\n","Epoch 53/100\n"," - 2s - loss: 3.3708e-07 - acc: 1.0000 - val_loss: 0.7431 - val_acc: 0.8708\n","Epoch 54/100\n"," - 2s - loss: 3.1165e-07 - acc: 1.0000 - val_loss: 0.7468 - val_acc: 0.8709\n","Epoch 55/100\n"," - 2s - loss: 2.8957e-07 - acc: 1.0000 - val_loss: 0.7503 - val_acc: 0.8710\n","Epoch 56/100\n"," - 2s - loss: 2.6992e-07 - acc: 1.0000 - val_loss: 0.7538 - val_acc: 0.8708\n","Epoch 57/100\n"," - 2s - loss: 2.5275e-07 - acc: 1.0000 - val_loss: 0.7572 - val_acc: 0.8708\n","Epoch 58/100\n"," - 2s - loss: 2.3659e-07 - acc: 1.0000 - val_loss: 0.7607 - val_acc: 0.8708\n","Epoch 59/100\n"," - 2s - loss: 2.2279e-07 - acc: 1.0000 - val_loss: 0.7640 - val_acc: 0.8710\n","Epoch 60/100\n"," - 2s - loss: 2.1006e-07 - acc: 1.0000 - val_loss: 0.7674 - val_acc: 0.8709\n","Epoch 61/100\n"," - 2s - loss: 1.9852e-07 - acc: 1.0000 - val_loss: 0.7706 - val_acc: 0.8708\n","Epoch 62/100\n"," - 2s - loss: 1.8838e-07 - acc: 1.0000 - val_loss: 0.7739 - val_acc: 0.8709\n","Epoch 63/100\n"," - 2s - loss: 1.7935e-07 - acc: 1.0000 - val_loss: 0.7771 - val_acc: 0.8708\n","Epoch 64/100\n"," - 2s - loss: 1.7119e-07 - acc: 1.0000 - val_loss: 0.7803 - val_acc: 0.8708\n","Epoch 65/100\n"," - 2s - loss: 1.6404e-07 - acc: 1.0000 - val_loss: 0.7835 - val_acc: 0.8708\n","Epoch 66/100\n"," - 2s - loss: 1.5744e-07 - acc: 1.0000 - val_loss: 0.7865 - val_acc: 0.8707\n","Epoch 67/100\n"," - 2s - loss: 1.5153e-07 - acc: 1.0000 - val_loss: 0.7896 - val_acc: 0.8708\n","Epoch 68/100\n"," - 2s - loss: 1.4647e-07 - acc: 1.0000 - val_loss: 0.7927 - val_acc: 0.8709\n","Epoch 69/100\n"," - 2s - loss: 1.4201e-07 - acc: 1.0000 - val_loss: 0.7955 - val_acc: 0.8707\n","Epoch 70/100\n"," - 2s - loss: 1.3792e-07 - acc: 1.0000 - val_loss: 0.7985 - val_acc: 0.8707\n","Epoch 71/100\n"," - 2s - loss: 1.3429e-07 - acc: 1.0000 - val_loss: 0.8013 - val_acc: 0.8708\n","Epoch 72/100\n"," - 2s - loss: 1.3096e-07 - acc: 1.0000 - val_loss: 0.8043 - val_acc: 0.8708\n","Epoch 73/100\n"," - 2s - loss: 1.2821e-07 - acc: 1.0000 - val_loss: 0.8070 - val_acc: 0.8708\n","Epoch 74/100\n"," - 2s - loss: 1.2574e-07 - acc: 1.0000 - val_loss: 0.8099 - val_acc: 0.8708\n","Epoch 75/100\n"," - 2s - loss: 1.2376e-07 - acc: 1.0000 - val_loss: 0.8127 - val_acc: 0.8706\n","Epoch 76/100\n"," - 2s - loss: 1.2182e-07 - acc: 1.0000 - val_loss: 0.8154 - val_acc: 0.8707\n","Epoch 77/100\n"," - 2s - loss: 1.2008e-07 - acc: 1.0000 - val_loss: 0.8180 - val_acc: 0.8708\n","Epoch 78/100\n"," - 2s - loss: 1.1845e-07 - acc: 1.0000 - val_loss: 0.8207 - val_acc: 0.8708\n","Epoch 79/100\n"," - 2s - loss: 1.1726e-07 - acc: 1.0000 - val_loss: 0.8232 - val_acc: 0.8707\n","Epoch 80/100\n"," - 2s - loss: 1.1627e-07 - acc: 1.0000 - val_loss: 0.8256 - val_acc: 0.8706\n","Epoch 81/100\n"," - 2s - loss: 1.1532e-07 - acc: 1.0000 - val_loss: 0.8282 - val_acc: 0.8701\n","Epoch 82/100\n"," - 2s - loss: 1.1445e-07 - acc: 1.0000 - val_loss: 0.8304 - val_acc: 0.8702\n","Epoch 83/100\n"," - 2s - loss: 1.1398e-07 - acc: 1.0000 - val_loss: 0.8329 - val_acc: 0.8701\n","Epoch 84/100\n"," - 2s - loss: 1.1323e-07 - acc: 1.0000 - val_loss: 0.8351 - val_acc: 0.8700\n","Epoch 85/100\n"," - 2s - loss: 1.1267e-07 - acc: 1.0000 - val_loss: 0.8376 - val_acc: 0.8700\n","Epoch 86/100\n"," - 2s - loss: 1.1234e-07 - acc: 1.0000 - val_loss: 0.8395 - val_acc: 0.8702\n","Epoch 87/100\n"," - 2s - loss: 1.1190e-07 - acc: 1.0000 - val_loss: 0.8414 - val_acc: 0.8700\n","Epoch 88/100\n"," - 2s - loss: 1.1158e-07 - acc: 1.0000 - val_loss: 0.8437 - val_acc: 0.8700\n","Epoch 89/100\n"," - 2s - loss: 1.1140e-07 - acc: 1.0000 - val_loss: 0.8458 - val_acc: 0.8698\n","Epoch 90/100\n"," - 2s - loss: 1.1110e-07 - acc: 1.0000 - val_loss: 0.8476 - val_acc: 0.8697\n","Epoch 91/100\n"," - 2s - loss: 1.1096e-07 - acc: 1.0000 - val_loss: 0.8495 - val_acc: 0.8697\n","Epoch 92/100\n"," - 2s - loss: 1.1076e-07 - acc: 1.0000 - val_loss: 0.8513 - val_acc: 0.8698\n","Epoch 93/100\n"," - 2s - loss: 1.1067e-07 - acc: 1.0000 - val_loss: 0.8529 - val_acc: 0.8699\n","Epoch 94/100\n"," - 2s - loss: 1.1045e-07 - acc: 1.0000 - val_loss: 0.8549 - val_acc: 0.8698\n","Epoch 95/100\n"," - 2s - loss: 1.1035e-07 - acc: 1.0000 - val_loss: 0.8565 - val_acc: 0.8700\n","Epoch 96/100\n"," - 2s - loss: 1.1028e-07 - acc: 1.0000 - val_loss: 0.8576 - val_acc: 0.8700\n","Epoch 97/100\n"," - 2s - loss: 1.1019e-07 - acc: 1.0000 - val_loss: 0.8594 - val_acc: 0.8699\n","Epoch 98/100\n"," - 2s - loss: 1.1012e-07 - acc: 1.0000 - val_loss: 0.8610 - val_acc: 0.8698\n","Epoch 99/100\n"," - 2s - loss: 1.1005e-07 - acc: 1.0000 - val_loss: 0.8624 - val_acc: 0.8700\n","Epoch 100/100\n"," - 2s - loss: 1.0998e-07 - acc: 1.0000 - val_loss: 0.8639 - val_acc: 0.8698\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f306a995a20>"]},"metadata":{"tags":[]},"execution_count":44}]}]}