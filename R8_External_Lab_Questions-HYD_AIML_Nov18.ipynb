{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"R8_External_Lab_Questions-HYD_AIML_Nov18.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QGIsF1ADyJ58"},"source":["# Transfer Learning CIFAR10"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"E-n6tVFayGBe"},"source":["* Train a simple convnet on the CIFAR dataset the first 5 output classes [0..4].\n","* Freeze convolutional layers and fine-tune dense layers for the last 5 ouput classes [5..9].\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Cq8ejXHJyGYq"},"source":["### 1. Import CIFAR10 data and create 2 datasets with one dataset having classes from 0 to 4 and other having classes from 5 to 9 "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uWYbxnBayFUP","colab":{}},"source":["import tensorflow as tf\n","tf.reset_default_graph()\n","tf.set_random_seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lPID3Jq2inSf","colab_type":"code","colab":{}},"source":["(Xtrain,Ytrain),(Xtest,Ytest) = tf.keras.datasets.cifar10.load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y15HS7wZpxo0","colab_type":"code","colab":{}},"source":["Ytrain = Ytrain.flatten()\n","Ytest = Ytest.flatten()\n","#Flatten the Y dataset as its not in one dimensional shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ph-aIZ-JjEt0","colab_type":"code","colab":{}},"source":["Xtrain_lt5 = Xtrain[Ytrain < 5]\n","Ytrain_lt5 = Ytrain[Ytrain < 5]\n","Xtest_lt5 = Xtest[Ytest < 5]\n","Ytest_lt5 = Ytest[Ytest < 5]\n","\n","Xtrain_gt5 = Xtrain[Ytrain >= 5]\n","Ytrain_gt5 = Ytrain[Ytrain >= 5] - 5  # make classes start at 0 for\n","Xtest_gt5 = Xtest[Ytest >= 5]         # np_utils.to_categorical\n","Ytest_gt5 = Ytest[Ytest >= 5] - 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xtCKmQh4yXhT"},"source":["### 2. Use One-hot encoding to divide y_train and y_test into required no of output classes"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uN5O2kJ3yYa6","colab":{}},"source":["Ytrain_lt5 = tf.keras.utils.to_categorical(Ytrain_lt5,num_classes=5)\n","Ytest_lt5 = tf.keras.utils.to_categorical(Ytest_lt5,num_classes=5)\n","Ytrain_gt5 = tf.keras.utils.to_categorical(Ytrain_gt5,num_classes=5)\n","Ytest_gt5 = tf.keras.utils.to_categorical(Ytest_gt5,num_classes=5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cuOiKWfeybAl"},"source":["### 3. Build a sequential neural network model which can classify the classes 0 to 4 of CIFAR10 dataset with at least 80% accuracy on test data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5HzxNbiiyoBD","colab":{}},"source":["# Normalize & change Dtype\n","Xtrain_lt5 = Xtrain_lt5.astype('float32') / 255\n","Xtest_lt5 = Xtest_lt5.astype('float32') / 255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uV3pYnrJroa4","colab_type":"code","colab":{}},"source":["Xtrain_lt5 = Xtrain_lt5.reshape(Xtrain_lt5.shape[0],32,32,3)\n","Xtest_lt5 = Xtest_lt5.reshape(Xtest_lt5.shape[0],32,32,3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUuE74AUsQF2","colab_type":"code","colab":{}},"source":["import keras\n","from keras.layers import Dense, Activation,Dropout, Flatten, Reshape\n","from keras.layers import Convolution2D, MaxPooling2D\n","from keras.models import Sequential\n","from keras.optimizers import Adam\n","from keras.layers.normalization import BatchNormalization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b6Szu0-tJwP_","colab_type":"code","colab":{}},"source":["model = Sequential()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5NFcHzSvKgdt","colab_type":"code","outputId":"5f67ca2a-e0e6-4cbd-9b6c-7548e5e82335","executionInfo":{"status":"ok","timestamp":1563108006367,"user_tz":-330,"elapsed":54472,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"colab":{"base_uri":"https://localhost:8080/","height":530}},"source":["# 1st Conv Layer\n","model.add(Convolution2D(64, 3, 3, input_shape=(32, 32, 3)))\n","model.add(Activation('relu'))\n","\n","# 2nd Conv Layer   \n","model.add(Convolution2D(64, 3, 3))\n","model.add(Activation('relu'))\n","\n","# Max Pooling\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","# Dropout Layer\n","model.add(Dropout(0.25))\n","\n","# 3rd Conv Layer   \n","model.add(Convolution2D(128, 3, 3))\n","model.add(Activation('relu'))\n","\n","# Max Pooling\n","model.add(MaxPooling2D(pool_size=(2,2)))\n","\n","# Fully Connected Layer\n","model.add(Flatten())\n","model.add(Dense(128))\n","model.add(Activation('relu'))\n","model.add(BatchNormalization())\n","  \n","# Prediction Layer\n","model.add(Dense(5))\n","model.add(Activation('softmax'))\n","    \n","# Loss and Optimizer\n","adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.0)\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","# Train the model\n","model.fit(Xtrain_lt5, Ytrain_lt5, batch_size=32, nb_epoch=10, \n","              validation_data=(Xtest_lt5, Ytest_lt5))"],"execution_count":294,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(32, 32, 3...)`\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3))`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3))`\n","  from ipykernel import kernelapp as app\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:37: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 25000 samples, validate on 5000 samples\n","Epoch 1/10\n","25000/25000 [==============================] - 5s 215us/step - loss: 0.5179 - acc: 0.8043 - val_loss: 0.5299 - val_acc: 0.8070\n","Epoch 2/10\n","25000/25000 [==============================] - 5s 203us/step - loss: 0.4802 - acc: 0.8186 - val_loss: 0.5651 - val_acc: 0.7940\n","Epoch 3/10\n","25000/25000 [==============================] - 5s 203us/step - loss: 0.4548 - acc: 0.8307 - val_loss: 0.5329 - val_acc: 0.8116\n","Epoch 4/10\n","25000/25000 [==============================] - 5s 203us/step - loss: 0.4228 - acc: 0.8421 - val_loss: 0.5318 - val_acc: 0.8142\n","Epoch 5/10\n","25000/25000 [==============================] - 5s 202us/step - loss: 0.3926 - acc: 0.8558 - val_loss: 0.5793 - val_acc: 0.8076\n","Epoch 6/10\n","25000/25000 [==============================] - 5s 203us/step - loss: 0.3531 - acc: 0.8714 - val_loss: 0.6094 - val_acc: 0.8080\n","Epoch 7/10\n","25000/25000 [==============================] - 5s 202us/step - loss: 0.2910 - acc: 0.8928 - val_loss: 0.7502 - val_acc: 0.7988\n","Epoch 8/10\n","25000/25000 [==============================] - 5s 203us/step - loss: 0.2320 - acc: 0.9156 - val_loss: 0.8364 - val_acc: 0.7866\n","Epoch 9/10\n","25000/25000 [==============================] - 5s 213us/step - loss: 0.1754 - acc: 0.9362 - val_loss: 0.8383 - val_acc: 0.7996\n","Epoch 10/10\n","25000/25000 [==============================] - 5s 208us/step - loss: 0.1286 - acc: 0.9527 - val_loss: 0.9650 - val_acc: 0.7956\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fadfc8422b0>"]},"metadata":{"tags":[]},"execution_count":294}]},{"cell_type":"code","metadata":{"id":"z0i9POvCL7ke","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":646},"outputId":"cbd6ca18-02a5-4670-cfc8-bce23c600f19","executionInfo":{"status":"ok","timestamp":1563108006369,"user_tz":-330,"elapsed":54447,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["model.summary()"],"execution_count":295,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1 (Conv2D)            (None, 30, 30, 64)        1792      \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 30, 30, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 28, 28, 64)        36928     \n","_________________________________________________________________\n","activation_2 (Activation)    (None, 28, 28, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_1 (MaxPooling2 (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 14, 14, 64)        0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n","_________________________________________________________________\n","activation_3 (Activation)    (None, 12, 12, 128)       0         \n","_________________________________________________________________\n","max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 4608)              0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 128)               589952    \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 128)               0         \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 128)               512       \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 5)                 645       \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 5)                 0         \n","=================================================================\n","Total params: 703,685\n","Trainable params: 703,429\n","Non-trainable params: 256\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1WYqBGdNaouz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":714},"outputId":"9384835e-f790-453c-9041-96cddd866978","executionInfo":{"status":"ok","timestamp":1563108841844,"user_tz":-330,"elapsed":298171,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","EPOCHS = 20\n","BS = 32\n","\n","# construct the training image generator for data augmentation\n","aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n","\twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n","\thorizontal_flip=True, fill_mode=\"nearest\")\n"," \n","# train the network\n","model.fit_generator(aug.flow(Xtrain_lt5, Ytrain_lt5, batch_size=BS),\n","\tvalidation_data=(Xtest_lt5, Ytest_lt5), steps_per_epoch=len(Xtrain_lt5) // BS,\n","\tepochs=EPOCHS)"],"execution_count":302,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6232 - acc: 0.7666 - val_loss: 0.5181 - val_acc: 0.8056\n","Epoch 2/20\n","781/781 [==============================] - 14s 19ms/step - loss: 0.6208 - acc: 0.7642 - val_loss: 0.5832 - val_acc: 0.7882\n","Epoch 3/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6169 - acc: 0.7671 - val_loss: 0.5965 - val_acc: 0.7832\n","Epoch 4/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.6136 - acc: 0.7690 - val_loss: 0.5470 - val_acc: 0.8010\n","Epoch 5/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6104 - acc: 0.7694 - val_loss: 0.6312 - val_acc: 0.7750\n","Epoch 6/20\n","781/781 [==============================] - 15s 20ms/step - loss: 0.6152 - acc: 0.7674 - val_loss: 0.4996 - val_acc: 0.8094\n","Epoch 7/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6076 - acc: 0.7718 - val_loss: 0.5513 - val_acc: 0.8002\n","Epoch 8/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6142 - acc: 0.7684 - val_loss: 0.5121 - val_acc: 0.8148\n","Epoch 9/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6064 - acc: 0.7729 - val_loss: 0.5260 - val_acc: 0.8118\n","Epoch 10/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6015 - acc: 0.7750 - val_loss: 0.5335 - val_acc: 0.8038\n","Epoch 11/20\n","781/781 [==============================] - 14s 19ms/step - loss: 0.6091 - acc: 0.7708 - val_loss: 0.5138 - val_acc: 0.8164\n","Epoch 12/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.6008 - acc: 0.7735 - val_loss: 0.5332 - val_acc: 0.8104\n","Epoch 13/20\n","781/781 [==============================] - 14s 19ms/step - loss: 0.5928 - acc: 0.7758 - val_loss: 0.5175 - val_acc: 0.8082\n","Epoch 14/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.5988 - acc: 0.7772 - val_loss: 0.5756 - val_acc: 0.7952\n","Epoch 15/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.5950 - acc: 0.7764 - val_loss: 0.4852 - val_acc: 0.8276\n","Epoch 16/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.5913 - acc: 0.7756 - val_loss: 0.5340 - val_acc: 0.8090\n","Epoch 17/20\n","781/781 [==============================] - 15s 20ms/step - loss: 0.5963 - acc: 0.7757 - val_loss: 0.5242 - val_acc: 0.8092\n","Epoch 18/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.5865 - acc: 0.7791 - val_loss: 0.5539 - val_acc: 0.8010\n","Epoch 19/20\n","781/781 [==============================] - 15s 20ms/step - loss: 0.5873 - acc: 0.7793 - val_loss: 0.5364 - val_acc: 0.8100\n","Epoch 20/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.5898 - acc: 0.7776 - val_loss: 0.5346 - val_acc: 0.8134\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fadfa12f4e0>"]},"metadata":{"tags":[]},"execution_count":302}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"woTfNst_ynRG"},"source":["### 4. In the model which was built above (for classification of classes 0-4 in CIFAR10), make only the dense layers to be trainable and conv layers to be non-trainable"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"o_VCDB3Byb1a","colab":{"base_uri":"https://localhost:8080/","height":782},"outputId":"6680c4e4-ccc1-47b1-f954-b18986404f25","executionInfo":{"status":"ok","timestamp":1563108854424,"user_tz":-330,"elapsed":1178,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["for layers in model.layers:\n","    print(layers.name)\n","    if('dense' not in layers.name):\n","        layers.trainable = False\n","        print(layers.name + 'is not trainable\\n')\n","    if('dense' in layers.name):\n","        print(layers.name + ' is trainable\\n')"],"execution_count":303,"outputs":[{"output_type":"stream","text":["conv2d_1\n","conv2d_1is not trainable\n","\n","activation_1\n","activation_1is not trainable\n","\n","conv2d_2\n","conv2d_2is not trainable\n","\n","activation_2\n","activation_2is not trainable\n","\n","max_pooling2d_1\n","max_pooling2d_1is not trainable\n","\n","dropout_1\n","dropout_1is not trainable\n","\n","conv2d_3\n","conv2d_3is not trainable\n","\n","activation_3\n","activation_3is not trainable\n","\n","max_pooling2d_2\n","max_pooling2d_2is not trainable\n","\n","flatten_1\n","flatten_1is not trainable\n","\n","dense_1\n","dense_1 is trainable\n","\n","activation_4\n","activation_4is not trainable\n","\n","batch_normalization_1\n","batch_normalization_1is not trainable\n","\n","dense_2\n","dense_2 is trainable\n","\n","activation_5\n","activation_5is not trainable\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1-uUPqWpyeyX"},"source":["### 5. Utilize the the model trained on CIFAR 10 (classes 0 to 4) to classify the classes 5 to 9 of CIFAR 10  (Use Transfer Learning) <br>\n","Achieve an accuracy of more than 85% on test data"]},{"cell_type":"code","metadata":{"id":"SsoloM4zWo4Z","colab_type":"code","colab":{}},"source":["# Reshape\n","Xtrain_gt5 = Xtrain_gt5.reshape(Xtrain_gt5.shape[0],32,32,3)\n","Xtest_gt5 = Xtest_gt5.reshape(Xtest_gt5.shape[0],32,32,3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"szHjJgDvyfCt","colab":{}},"source":["# Normalize & change Dtype\n","Xtrain_gt5 = Xtrain_gt5.astype('float32') / 255\n","Xtest_gt5 = Xtest_gt5.astype('float32') / 255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOcddvSEWj5m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":768},"outputId":"f84b0d99-3176-4ad6-9820-58d597061ca8","executionInfo":{"status":"ok","timestamp":1563113605615,"user_tz":-330,"elapsed":291216,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["#model.fit(Xtrain_gt5, Ytrain_gt5, batch_size=32, nb_epoch=10, \n","#              validation_data=(Xtest_gt5, Ytest_gt5))\n","\n","#Using the weights of previous model and augemting the data to get better accuracy\n","\n","EPOCHS = 20\n","BS = 32\n","\n","# construct the training image generator for data augmentation\n","aug1 = ImageDataGenerator(rotation_range=20, zoom_range=0.25,\n","\twidth_shift_range=0.1, height_shift_range=0.1, shear_range=0.15,\n","\thorizontal_flip=True, fill_mode=\"nearest\")\n"," \n","# train the network\n","model.fit_generator(aug1.flow(Xtrain_gt5, Ytrain_gt5, batch_size=BS),\n","\tvalidation_data=(Xtest_gt5, Ytest_gt5), steps_per_epoch=len(Xtrain_gt5) // BS,\n","\tepochs=EPOCHS)"],"execution_count":389,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n"," 13/781 [..............................] - ETA: 11s - loss: 0.3881 - acc: 0.8582"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["781/781 [==============================] - 14s 18ms/step - loss: 0.4365 - acc: 0.8443 - val_loss: 0.4314 - val_acc: 0.8458\n","Epoch 2/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.4285 - acc: 0.8447 - val_loss: 0.5067 - val_acc: 0.8212\n","Epoch 3/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.4222 - acc: 0.8478 - val_loss: 0.3933 - val_acc: 0.8572\n","Epoch 4/20\n","781/781 [==============================] - 14s 19ms/step - loss: 0.4235 - acc: 0.8476 - val_loss: 0.4288 - val_acc: 0.8496\n","Epoch 5/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.4145 - acc: 0.8494 - val_loss: 0.3900 - val_acc: 0.8646\n","Epoch 6/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.4096 - acc: 0.8515 - val_loss: 0.4369 - val_acc: 0.8458\n","Epoch 7/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.4025 - acc: 0.8535 - val_loss: 0.4120 - val_acc: 0.8552\n","Epoch 8/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.4124 - acc: 0.8496 - val_loss: 0.4725 - val_acc: 0.8392\n","Epoch 9/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3970 - acc: 0.8535 - val_loss: 0.4072 - val_acc: 0.8562\n","Epoch 10/20\n","781/781 [==============================] - 15s 20ms/step - loss: 0.3955 - acc: 0.8561 - val_loss: 0.3631 - val_acc: 0.8736\n","Epoch 11/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3995 - acc: 0.8532 - val_loss: 0.3564 - val_acc: 0.8740\n","Epoch 12/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3971 - acc: 0.8557 - val_loss: 0.3662 - val_acc: 0.8726\n","Epoch 13/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.3924 - acc: 0.8575 - val_loss: 0.4354 - val_acc: 0.8546\n","Epoch 14/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3899 - acc: 0.8589 - val_loss: 0.3232 - val_acc: 0.8870\n","Epoch 15/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3880 - acc: 0.8583 - val_loss: 0.3270 - val_acc: 0.8878\n","Epoch 16/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.3865 - acc: 0.8610 - val_loss: 0.4075 - val_acc: 0.8582\n","Epoch 17/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.3842 - acc: 0.8606 - val_loss: 0.3703 - val_acc: 0.8696\n","Epoch 18/20\n","781/781 [==============================] - 14s 18ms/step - loss: 0.3864 - acc: 0.8596 - val_loss: 0.3545 - val_acc: 0.8750\n","Epoch 19/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3824 - acc: 0.8624 - val_loss: 0.4228 - val_acc: 0.8518\n","Epoch 20/20\n","781/781 [==============================] - 15s 19ms/step - loss: 0.3799 - acc: 0.8622 - val_loss: 0.3485 - val_acc: 0.8806\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fadef453f98>"]},"metadata":{"tags":[]},"execution_count":389}]},{"cell_type":"code","metadata":{"id":"5iUY2R0e1USg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"9602f7e3-df38-4d04-99c3-72c034700b62","executionInfo":{"status":"ok","timestamp":1563113682979,"user_tz":-330,"elapsed":1002,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["score = model.evaluate(Xtest_gt5, Ytest_gt5, batch_size=128, verbose=0)\n","print(score)"],"execution_count":390,"outputs":[{"output_type":"stream","text":["[0.3485453933954239, 0.8806]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pAwF4f18ly54","colab_type":"code","colab":{}},"source":["# Data augmentation didnt help much. My accuracy improved only marginally by doing Augumentation.\n","# I would like to understand where i am going wrong and what tuning of hyper parameter would help#"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FU-HwvIdH0M-"},"source":["## Sentiment analysis <br> \n","\n","The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n","Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nAQDiZHRH0M_"},"source":["### 6. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"]},{"cell_type":"code","metadata":{"id":"cJ00LN9RlDby","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"7c366bca-55ac-4fa3-e8ee-d79759cbd113","executionInfo":{"status":"ok","timestamp":1563109454975,"user_tz":-330,"elapsed":67466,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":307,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3eXGIe-SH0NA","colab":{}},"source":["import pandas as pd\n","data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/R8/tweets.csv', encoding = \"ISO-8859-1\").dropna()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"CWeWe1eJH0NF","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3a62152e-14d5-40c2-81e8-43847b57e76c","executionInfo":{"status":"ok","timestamp":1563109548611,"user_tz":-330,"elapsed":926,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["data.shape"],"execution_count":310,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3291, 3)"]},"metadata":{"tags":[]},"execution_count":310}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7kX-WoJDH0NV","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"ec64a91e-ae13-413e-c662-10ee0ab15f21","executionInfo":{"status":"ok","timestamp":1563109645436,"user_tz":-330,"elapsed":1004,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["data.head()"],"execution_count":311,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_text</th>\n","      <th>emotion_in_tweet_is_directed_at</th>\n","      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n","      <td>iPhone</td>\n","      <td>Negative emotion</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Positive emotion</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n","      <td>iPad</td>\n","      <td>Positive emotion</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>@sxsw I hope this year's festival isn't as cra...</td>\n","      <td>iPad or iPhone App</td>\n","      <td>Negative emotion</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n","      <td>Google</td>\n","      <td>Positive emotion</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                          tweet_text  ... is_there_an_emotion_directed_at_a_brand_or_product\n","0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  ...                                   Negative emotion\n","1  @jessedee Know about @fludapp ? Awesome iPad/i...  ...                                   Positive emotion\n","2  @swonderlin Can not wait for #iPad 2 also. The...  ...                                   Positive emotion\n","3  @sxsw I hope this year's festival isn't as cra...  ...                                   Negative emotion\n","4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  ...                                   Positive emotion\n","\n","[5 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":311}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OGWB3P2WH0NY"},"source":["### Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bdgA_8N2H0NY","colab":{}},"source":["data = data[(data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Positive emotion') | (data['is_there_an_emotion_directed_at_a_brand_or_product'] == 'Negative emotion')]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_Jlu-reIH0Na","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"49aeeb9a-1440-4669-b2db-25f144ab37ef","executionInfo":{"status":"ok","timestamp":1563109714601,"user_tz":-330,"elapsed":993,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["data.shape"],"execution_count":313,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3191, 3)"]},"metadata":{"tags":[]},"execution_count":313}]},{"cell_type":"markdown","metadata":{"id":"g08IHfWPmZaH","colab_type":"text"},"source":["In total 100 records were dropped as those were neither positive nor negative response"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SotCRvkDH0Nf"},"source":["### 7. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n","\n","#### Use `vect` as the variable name for initialising CountVectorizer."]},{"cell_type":"markdown","metadata":{"id":"qol95eW5oFRN","colab_type":"text"},"source":["Divide the data into two dataset with X as Tweet and Y as positive/Negative"]},{"cell_type":"code","metadata":{"id":"WtVeKwiyn4uA","colab_type":"code","colab":{}},"source":["X = data['tweet_text']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"snF_2GAwtNJc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6532f3fc-c560-4276-fc0c-0b6d55bcf1d1","executionInfo":{"status":"ok","timestamp":1563111519285,"user_tz":-330,"elapsed":1072,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["X.shape"],"execution_count":349,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3191,)"]},"metadata":{"tags":[]},"execution_count":349}]},{"cell_type":"code","metadata":{"id":"co7Fw50jn-pG","colab_type":"code","colab":{}},"source":["Y = data['is_there_an_emotion_directed_at_a_brand_or_product']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YcbkY4sgH0Ng","colab":{}},"source":["from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KyXtZGr-H0Nl","colab":{}},"source":["# Term Frequency\n","vect = CountVectorizer()\n","tf = vect.fit_transform(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5pxd5fSHH0Nt"},"source":["### 8. Find number of different words in vocabulary"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"p1DQ2LdNH0Nu","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"1a9c24ba-20b0-43ca-b174-2419a3b739f1","executionInfo":{"status":"ok","timestamp":1563110697103,"user_tz":-330,"elapsed":2186,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["tf.shape"],"execution_count":333,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3191, 5648)"]},"metadata":{"tags":[]},"execution_count":333}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dwtgjTBeH0Ny"},"source":["#### Tip: To see all available functions for an Object use dir"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2n_iCcTNH0N0","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"8f9a5296-3cb0-423a-c477-2f2cd9cef805","executionInfo":{"status":"ok","timestamp":1563110737713,"user_tz":-330,"elapsed":2344,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["dir(tf)"],"execution_count":335,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['__abs__',\n"," '__add__',\n"," '__array_priority__',\n"," '__bool__',\n"," '__class__',\n"," '__delattr__',\n"," '__dict__',\n"," '__dir__',\n"," '__div__',\n"," '__doc__',\n"," '__eq__',\n"," '__format__',\n"," '__ge__',\n"," '__getattr__',\n"," '__getattribute__',\n"," '__getitem__',\n"," '__gt__',\n"," '__hash__',\n"," '__iadd__',\n"," '__idiv__',\n"," '__imul__',\n"," '__init__',\n"," '__init_subclass__',\n"," '__isub__',\n"," '__iter__',\n"," '__itruediv__',\n"," '__le__',\n"," '__len__',\n"," '__lt__',\n"," '__matmul__',\n"," '__module__',\n"," '__mul__',\n"," '__ne__',\n"," '__neg__',\n"," '__new__',\n"," '__nonzero__',\n"," '__pow__',\n"," '__radd__',\n"," '__rdiv__',\n"," '__reduce__',\n"," '__reduce_ex__',\n"," '__repr__',\n"," '__rmatmul__',\n"," '__rmul__',\n"," '__rsub__',\n"," '__rtruediv__',\n"," '__setattr__',\n"," '__setitem__',\n"," '__sizeof__',\n"," '__str__',\n"," '__sub__',\n"," '__subclasshook__',\n"," '__truediv__',\n"," '__weakref__',\n"," '_add_dense',\n"," '_add_sparse',\n"," '_arg_min_or_max',\n"," '_arg_min_or_max_axis',\n"," '_asindices',\n"," '_binopt',\n"," '_cs_matrix__get_has_canonical_format',\n"," '_cs_matrix__get_sorted',\n"," '_cs_matrix__set_has_canonical_format',\n"," '_cs_matrix__set_sorted',\n"," '_deduped_data',\n"," '_divide',\n"," '_divide_sparse',\n"," '_get_arrayXarray',\n"," '_get_arrayXint',\n"," '_get_arrayXslice',\n"," '_get_columnXarray',\n"," '_get_dtype',\n"," '_get_intXarray',\n"," '_get_intXint',\n"," '_get_intXslice',\n"," '_get_sliceXarray',\n"," '_get_sliceXint',\n"," '_get_sliceXslice',\n"," '_get_submatrix',\n"," '_imag',\n"," '_inequality',\n"," '_insert_many',\n"," '_major_index_fancy',\n"," '_major_slice',\n"," '_maximum_minimum',\n"," '_min_or_max',\n"," '_min_or_max_axis',\n"," '_minor_index_fancy',\n"," '_minor_reduce',\n"," '_minor_slice',\n"," '_mul_multivector',\n"," '_mul_scalar',\n"," '_mul_sparse_matrix',\n"," '_mul_vector',\n"," '_prepare_indices',\n"," '_process_toarray_args',\n"," '_real',\n"," '_rsub_dense',\n"," '_scalar_binopt',\n"," '_set_arrayXarray',\n"," '_set_arrayXarray_sparse',\n"," '_set_dtype',\n"," '_set_intXint',\n"," '_set_many',\n"," '_set_self',\n"," '_setdiag',\n"," '_shape',\n"," '_sub_dense',\n"," '_sub_sparse',\n"," '_swap',\n"," '_validate_indices',\n"," '_with_data',\n"," '_zero_many',\n"," 'arcsin',\n"," 'arcsinh',\n"," 'arctan',\n"," 'arctanh',\n"," 'argmax',\n"," 'argmin',\n"," 'asformat',\n"," 'asfptype',\n"," 'astype',\n"," 'ceil',\n"," 'check_format',\n"," 'conj',\n"," 'conjugate',\n"," 'copy',\n"," 'count_nonzero',\n"," 'data',\n"," 'deg2rad',\n"," 'diagonal',\n"," 'dot',\n"," 'dtype',\n"," 'eliminate_zeros',\n"," 'expm1',\n"," 'floor',\n"," 'format',\n"," 'getH',\n"," 'get_shape',\n"," 'getcol',\n"," 'getformat',\n"," 'getmaxprint',\n"," 'getnnz',\n"," 'getrow',\n"," 'has_canonical_format',\n"," 'has_sorted_indices',\n"," 'indices',\n"," 'indptr',\n"," 'log1p',\n"," 'max',\n"," 'maximum',\n"," 'maxprint',\n"," 'mean',\n"," 'min',\n"," 'minimum',\n"," 'multiply',\n"," 'ndim',\n"," 'nnz',\n"," 'nonzero',\n"," 'power',\n"," 'prune',\n"," 'rad2deg',\n"," 'reshape',\n"," 'resize',\n"," 'rint',\n"," 'set_shape',\n"," 'setdiag',\n"," 'shape',\n"," 'sign',\n"," 'sin',\n"," 'sinh',\n"," 'sort_indices',\n"," 'sorted_indices',\n"," 'sqrt',\n"," 'sum',\n"," 'sum_duplicates',\n"," 'tan',\n"," 'tanh',\n"," 'toarray',\n"," 'tobsr',\n"," 'tocoo',\n"," 'tocsc',\n"," 'tocsr',\n"," 'todense',\n"," 'todia',\n"," 'todok',\n"," 'tolil',\n"," 'transpose',\n"," 'trunc']"]},"metadata":{"tags":[]},"execution_count":335}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ShA6D8jKH0N5"},"source":["### Find out how many Positive and Negative emotions are there.\n","\n","Hint: Use value_counts on that column"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"q7LAl5pzH0N6","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"f9da4df9-cedc-4a73-fffc-c7f8ddd25490","executionInfo":{"status":"ok","timestamp":1563110760787,"user_tz":-330,"elapsed":987,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["pd.value_counts(data['is_there_an_emotion_directed_at_a_brand_or_product'])"],"execution_count":336,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Positive emotion    2672\n","Negative emotion     519\n","Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"]},"metadata":{"tags":[]},"execution_count":336}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"IUvgj0FoH0N9"},"source":["###  Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'label'\n","\n","Hint: use map on that column and give labels"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"YftKwFv7H0N9","colab":{}},"source":["data['label'] = data.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1, 'Negative emotion':0})"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3YErwYLCH0N_"},"source":["### 9. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"]},{"cell_type":"code","metadata":{"id":"svqtObusrD3m","colab_type":"code","colab":{}},"source":["x = data.tweet_text\n","y = data['label']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lNkwrGgEH0OA","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","xtrain,xtest,ytrain,ytest = train_test_split(x,y, random_state=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Q5nlCuaaH0OD"},"source":["## 10. **Predicting the sentiment:**\n","\n","\n","### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"]},{"cell_type":"code","metadata":{"id":"8k_B7ItQr8Bw","colab_type":"code","colab":{}},"source":["vect = CountVectorizer(ngram_range=(1, 1))\n","X_train_dtm = vect.fit_transform(xtrain)\n","X_test_dtm = vect.transform(xtest)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HQCPgBwu81O","colab_type":"code","colab":{}},"source":["from sklearn.naive_bayes import MultinomialNB\n","nb = MultinomialNB()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qvZd8AgrvbOV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"189d4585-0ee9-4cc2-f4c8-cce2bb0f03fc","executionInfo":{"status":"ok","timestamp":1563112143434,"user_tz":-330,"elapsed":1032,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["nb.fit(X_train_dtm,ytrain)"],"execution_count":364,"outputs":[{"output_type":"execute_result","data":{"text/plain":["MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"]},"metadata":{"tags":[]},"execution_count":364}]},{"cell_type":"code","metadata":{"id":"HwlMSRM8vpT5","colab_type":"code","colab":{}},"source":["y_pred = nb.predict(X_test_dtm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LR8pC2wHv2fN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e24f2086-d89a-492b-85a4-2e486187eea8","executionInfo":{"status":"ok","timestamp":1563112277632,"user_tz":-330,"elapsed":1204,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["from sklearn import metrics\n","metrics.accuracy_score(ytest, y_pred)"],"execution_count":367,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.8471177944862155"]},"metadata":{"tags":[]},"execution_count":367}]},{"cell_type":"code","metadata":{"id":"bZh-_s6FwOH0","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","lr = LogisticRegression()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c5F77qhowcle","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"95c7208a-37da-4025-bcb1-1f5576a009fb","executionInfo":{"status":"ok","timestamp":1563112516648,"user_tz":-330,"elapsed":967,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["lr.fit(X_train_dtm,ytrain)"],"execution_count":375,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='warn', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{"tags":[]},"execution_count":375}]},{"cell_type":"code","metadata":{"id":"hucxmhnRwnn9","colab_type":"code","colab":{}},"source":["y_pred_lr = lr.predict(X_test_dtm)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VN4BZvolwsyl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5aa84081-dd8c-4c55-9273-973760fc67c4","executionInfo":{"status":"ok","timestamp":1563112527296,"user_tz":-330,"elapsed":1193,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["metrics.accuracy_score(ytest, y_pred_lr)"],"execution_count":377,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.868421052631579"]},"metadata":{"tags":[]},"execution_count":377}]},{"cell_type":"markdown","metadata":{"id":"VBkRovSjxHWB","colab_type":"text"},"source":["Logistic Regression is giving better accuracy. I believe applying ensemble technique can further improve the overall accuracy of this model."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"sw-0B33tH0Ox"},"source":["## 11. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"okCTOs1TH0Oy","colab":{}},"source":["def tokenize_test(vect):\n","    x_train_dtm = vect.fit_transform(xtrain)\n","    print('Features: ', x_train_dtm.shape[1])\n","    x_test_dtm = vect.transform(xtest)\n","    nb = MultinomialNB()\n","    nb.fit(x_train_dtm, ytrain)\n","    y_pred_class = nb.predict(x_test_dtm)\n","    print('Accuracy: ', metrics.accuracy_score(ytest, y_pred_class))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JxZ8jfPEH0O0"},"source":["### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kdCyAN_IH0O0","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"0e58076d-1bde-45de-e863-1baa707aeec5","executionInfo":{"status":"ok","timestamp":1563112748510,"user_tz":-330,"elapsed":1088,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["# include 1-grams and 2-grams\n","vect = CountVectorizer(ngram_range=(1, 2))\n","tokenize_test(vect)"],"execution_count":381,"outputs":[{"output_type":"stream","text":["Features:  24855\n","Accuracy:  0.8558897243107769\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"axepytmgH0O4"},"source":["### 12. Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HToGkq7vH0O4","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"447b00b2-8858-46b7-abf5-ecf79111f906","executionInfo":{"status":"ok","timestamp":1563112831761,"user_tz":-330,"elapsed":1048,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["vect1 = CountVectorizer(stop_words='english')\n","tokenize_test(vect1)"],"execution_count":382,"outputs":[{"output_type":"stream","text":["Features:  4681\n","Accuracy:  0.8533834586466166\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iOIlJRxoH0O7"},"source":["### 13. Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"6fUhff-oH0O8","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"ef93202b-83f4-4a15-f09c-fc47d80e9d75","executionInfo":{"status":"ok","timestamp":1563112868299,"user_tz":-330,"elapsed":1140,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["vect2 = CountVectorizer(stop_words='english',max_features =300)\n","tokenize_test(vect2)"],"execution_count":383,"outputs":[{"output_type":"stream","text":["Features:  300\n","Accuracy:  0.8107769423558897\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"S2KZNWVkH0PA"},"source":["### 14. Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3v9XD082H0PB","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"52d30b46-61c6-4405-aa0a-9e502176cb0b","executionInfo":{"status":"ok","timestamp":1563112896197,"user_tz":-330,"elapsed":1198,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["vect3 = CountVectorizer(ngram_range=(1, 2),max_features =15000)\n","tokenize_test(vect3)"],"execution_count":384,"outputs":[{"output_type":"stream","text":["Features:  15000\n","Accuracy:  0.8533834586466166\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"We3JK_SRH0PO"},"source":["### 15. Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fUHrfDCyH0PP","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"5537238b-847d-4062-8d50-75d0e3229ba4","executionInfo":{"status":"ok","timestamp":1563112925800,"user_tz":-330,"elapsed":930,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["vect4 = CountVectorizer(ngram_range=(1, 2),min_df = 2)\n","tokenize_test(vect4)"],"execution_count":385,"outputs":[{"output_type":"stream","text":["Features:  7764\n","Accuracy:  0.8583959899749374\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p40rLyEjymZM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xKbD5-PVynb4","colab_type":"text"},"source":["##Bonus Code##\n","Below i am creating a function with LogisticRegression and using all the above vector function and pass it to the tokenize predictor to check accuracy\n"]},{"cell_type":"code","metadata":{"id":"bopMuHpeynvU","colab_type":"code","colab":{}},"source":["def tokenize_test_lr(vect):\n","    x_train_dtm = vect.fit_transform(xtrain)\n","    print('Features: ', x_train_dtm.shape[1])\n","    x_test_dtm = vect.transform(xtest)\n","    lr = LogisticRegression()\n","    lr.fit(x_train_dtm, ytrain)\n","    y_pred_lr = lr.predict(x_test_dtm)\n","    print('Accuracy: ', metrics.accuracy_score(ytest, y_pred_lr))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8rFauHcLzLLl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":105},"outputId":"6ba873db-8f98-40f2-c6c7-87498b034e4b","executionInfo":{"status":"ok","timestamp":1563113184225,"user_tz":-330,"elapsed":966,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["vect5 = CountVectorizer(ngram_range=(1, 2),min_df = 2,stop_words='english',max_features =15000)\n","tokenize_test_lr(vect5)"],"execution_count":387,"outputs":[{"output_type":"stream","text":["Features:  5451\n","Accuracy:  0.8671679197994987\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n","  FutureWarning)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"muADuza7zler","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"34bcd25a-4ad1-4046-95de-0c290c85d101","executionInfo":{"status":"ok","timestamp":1563113213283,"user_tz":-330,"elapsed":1034,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["tokenize_test(vect5)"],"execution_count":388,"outputs":[{"output_type":"stream","text":["Features:  5451\n","Accuracy:  0.8659147869674185\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EPdLA0SMzw6F","colab_type":"text"},"source":["## Both Logistic Regression and Naive Bayes are giving almost similiar accuracy\n"]}]}