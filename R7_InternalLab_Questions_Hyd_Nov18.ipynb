{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"R7_InternalLab_Questions_Hyd_Nov18.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MyfMmMnPJjvn"},"source":["## Train a simple convnet on the Fashion MNIST dataset"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zjcGOJhcJjvp"},"source":["In this, we will see how to deal with image data and train a convnet for image classification task."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"jR0Pl2XjJjvq"},"source":["### Load the  `fashion_mnist`  dataset\n","\n","** Use keras.datasets to load the dataset **"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Qr75v_UYJjvs","colab":{"base_uri":"https://localhost:8080/","height":170},"outputId":"17c86bd7-34a7-4115-fbb9-aef496c65a1a","executionInfo":{"status":"ok","timestamp":1560076684327,"user_tz":-330,"elapsed":2922,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["from keras.datasets import fashion_mnist\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","32768/29515 [=================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","26427392/26421880 [==============================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","8192/5148 [===============================================] - 0s 0us/step\n","Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","4423680/4422102 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hTI42-0qJjvw"},"source":["### Find no.of samples are there in training and test datasets"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"g2sf67VoJjvx","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"310654f9-0e30-4884-ba03-29027544eb65","executionInfo":{"status":"ok","timestamp":1560076689746,"user_tz":-330,"elapsed":1221,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["print ('shape of training dataset', x_train.shape)\n","print ('shape of testing dataset',x_test.shape)\n","print ('Classes of output dataset', y_train.shape)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["shape of training dataset (60000, 28, 28)\n","shape of testing dataset (10000, 28, 28)\n","Classes of output dataset (60000,)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WytT2eRnJjv4"},"source":["### Find dimensions of an image in the dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"d8ad2eaa-8042-4317-ad33-f14e84974e9d","executionInfo":{"status":"ok","timestamp":1560076692506,"user_tz":-330,"elapsed":1231,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}},"id":"zCh2ScLtmSUk","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import numpy as np\n","np.bincount(y_train).shape"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(10,)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"WKB5Df_pmmLv","colab_type":"text"},"source":["# The above output shows that there are 10 clases in the output dataset"]},{"cell_type":"code","metadata":{"id":"y4jXwFKWmUeR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"8ea0dbb1-1a14-42f6-b747-b69f064c7c3e","executionInfo":{"status":"ok","timestamp":1560076702865,"user_tz":-330,"elapsed":1285,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["x_train[0].shape"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28, 28)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"5jtdZ7RqJjv8"},"source":["### Convert train and test labels to one hot vectors\n","\n","** check `keras.utils.to_categorical()` **"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sAD3q5I6Jjv9","colab":{}},"source":["import tensorflow as tf\n","y_train = tf.keras.utils.to_categorical(y_train, num_classes = 10)\n","y_test = tf.keras.utils.to_categorical(y_test, num_classes = 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xO5BRBzBJjwD"},"source":["### Normalize both the train and test image data from 0-255 to 0-1"]},{"cell_type":"code","metadata":{"id":"5gAZlpJNqBg3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"74c73561-dfa6-4f03-ad3a-9176e0ef8f66","executionInfo":{"status":"ok","timestamp":1560076711249,"user_tz":-330,"elapsed":1346,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["x_train.dtype"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["dtype('uint8')"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"eMlH0vm5qQqZ","colab_type":"text"},"source":["#changing the datatype to float else all values will be 0 except 255"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3fUQpMHxJjwE","colab":{}},"source":["x_train = x_train.astype(\"float32\")/255"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Okwo_SB5JjwI","colab":{}},"source":["x_test = x_test.astype(\"float32\")/255"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"da5-DwgrJjwM"},"source":["### Reshape the data from 28x28 to 28x28x1 to match input dimensions in Conv2D layer in keras"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LPGVQ-JJJjwN","colab":{}},"source":["x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5v5snhuXsqP3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f50e7118-92b3-4e3c-d2d6-554c60b8fc3e","executionInfo":{"status":"ok","timestamp":1560076734565,"user_tz":-330,"elapsed":1268,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["x_train[0].shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(28, 28, 1)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"ye2qDrgGvn1X","colab_type":"code","colab":{}},"source":["x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OFRRTJq8JjwQ"},"source":["### Import the necessary layers from keras to build the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dWTZYnKSJjwR","colab":{}},"source":["import keras\n","from keras.models import Sequential\n","from keras.layers import Dense, Activation, Dropout, Flatten, Reshape\n","from keras.layers import Convolution2D, MaxPooling2D\n","from keras.utils import np_utils"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C18AoS7eJjwU"},"source":["### Build a model \n","\n","** with 2 Conv layers having `32 3*3 filters` in both convolutions with `relu activations` and `flatten` before passing the feature map into 2 fully connected layers (or Dense Layers) having 128 and 10 neurons with `relu` and `softmax` activations respectively. Now, using `categorical_crossentropy` loss with `adam` optimizer train the model with early stopping `patience=5` and no.of `epochs=10`. **"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DORCLgSwJjwV","colab":{"base_uri":"https://localhost:8080/","height":598},"outputId":"37f51081-f7e9-4824-e399-c88001bf7b42","executionInfo":{"status":"ok","timestamp":1560076869837,"user_tz":-330,"elapsed":110696,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["  # Define model\n","    model2 = Sequential()\n","    \n","    # 1st Conv Layer\n","    model2.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n","    model2.add(Activation('relu'))\n","\n","    # 2nd Conv Layer\n","    model2.add(Convolution2D(32, 3, 3))\n","    model2.add(Activation('relu'))\n","\n","    # Fully Connected Layer\n","    model2.add(Flatten())\n","    model2.add(Dense(128))\n","    model2.add(Activation('relu'))\n","\n","    # Prediction Layer\n","    model2.add(Dense(10))\n","    model2.add(Activation('softmax'))\n","\n","    # Loss and Optimizer\n","    model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    # Store Training Results\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n","    callback_list = [early_stopping]\n","\n","    # Train the model2\n","    model2.fit(x_train, y_train, batch_size=32, nb_epoch=10, \n","              validation_data=(x_test, y_test), callbacks=callback_list)\n","    \n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Colocations handled automatically by placer.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.cast instead.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/10\n","60000/60000 [==============================] - 16s 273us/step - loss: 0.3717 - acc: 0.8660 - val_loss: 0.2866 - val_acc: 0.8940\n","Epoch 2/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.2301 - acc: 0.9150 - val_loss: 0.2610 - val_acc: 0.9079\n","Epoch 3/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.1690 - acc: 0.9374 - val_loss: 0.2606 - val_acc: 0.9132\n","Epoch 4/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.1200 - acc: 0.9553 - val_loss: 0.2899 - val_acc: 0.9121\n","Epoch 5/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.0813 - acc: 0.9693 - val_loss: 0.3056 - val_acc: 0.9103\n","Epoch 6/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.0578 - acc: 0.9791 - val_loss: 0.3266 - val_acc: 0.9152\n","Epoch 7/10\n","60000/60000 [==============================] - 10s 172us/step - loss: 0.0389 - acc: 0.9861 - val_loss: 0.3904 - val_acc: 0.9175\n","Epoch 8/10\n","60000/60000 [==============================] - 10s 173us/step - loss: 0.0304 - acc: 0.9886 - val_loss: 0.4112 - val_acc: 0.9156\n","Epoch 9/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.0237 - acc: 0.9916 - val_loss: 0.4911 - val_acc: 0.9123\n","Epoch 10/10\n","60000/60000 [==============================] - 10s 171us/step - loss: 0.0189 - acc: 0.9935 - val_loss: 0.4949 - val_acc: 0.9137\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe557d56eb8>"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ju69vKdIJjwX"},"source":["### Now, to the above model add `max` pooling layer of `filter size 2x2` and `dropout` layer with `p=0.25` after the 2 conv layers and run the model"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L2hAP94vJjwY","colab":{"base_uri":"https://localhost:8080/","height":547},"outputId":"2b25769f-0ead-4de3-9866-321f74eca4a9","executionInfo":{"status":"ok","timestamp":1560077325261,"user_tz":-330,"elapsed":84622,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["    # Define model\n","    model3 = Sequential()\n","    \n","    # 1st Conv Layer\n","    model3.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n","    model3.add(Activation('relu'))\n","\n","    # 2nd Conv Layer\n","    model3.add(Convolution2D(32, 3, 3))\n","    model3.add(Activation('relu'))\n","\n","    # Max Pooling\n","    model3.add(MaxPooling2D(pool_size=(2,2)))\n","    \n","    # Dropout\n","    model3.add(Dropout(0.25))\n","    \n","    # Fully Connected Layer\n","    model3.add(Flatten())\n","    model3.add(Dense(128))\n","    model3.add(Activation('relu'))\n","\n","    # Prediction Layer\n","    model3.add(Dense(10))\n","    model3.add(Activation('softmax'))\n","\n","    # Loss and Optimizer\n","    model3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    # Store Training Results\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n","    callback_list = [early_stopping]\n","\n","    # Train the model3\n","    model3.fit(x_train, y_train, batch_size=32, nb_epoch=10, \n","              validation_data=(x_test, y_test), callbacks=callback_list)\n","    "],"execution_count":14,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Train on 60000 samples, validate on 10000 samples\n","Epoch 1/10\n","60000/60000 [==============================] - 9s 146us/step - loss: 0.3986 - acc: 0.8560 - val_loss: 0.2984 - val_acc: 0.8922\n","Epoch 2/10\n","60000/60000 [==============================] - 8s 138us/step - loss: 0.2593 - acc: 0.9051 - val_loss: 0.2578 - val_acc: 0.9047\n","Epoch 3/10\n","60000/60000 [==============================] - 8s 137us/step - loss: 0.2101 - acc: 0.9214 - val_loss: 0.2382 - val_acc: 0.9138\n","Epoch 4/10\n","60000/60000 [==============================] - 8s 138us/step - loss: 0.1755 - acc: 0.9347 - val_loss: 0.2134 - val_acc: 0.9217\n","Epoch 5/10\n","60000/60000 [==============================] - 8s 138us/step - loss: 0.1484 - acc: 0.9440 - val_loss: 0.2268 - val_acc: 0.9232\n","Epoch 6/10\n","60000/60000 [==============================] - 8s 138us/step - loss: 0.1244 - acc: 0.9535 - val_loss: 0.2455 - val_acc: 0.9209\n","Epoch 7/10\n","60000/60000 [==============================] - 8s 137us/step - loss: 0.1060 - acc: 0.9599 - val_loss: 0.2377 - val_acc: 0.9253\n","Epoch 8/10\n","60000/60000 [==============================] - 8s 138us/step - loss: 0.0887 - acc: 0.9662 - val_loss: 0.2487 - val_acc: 0.9225\n","Epoch 9/10\n","60000/60000 [==============================] - 8s 137us/step - loss: 0.0779 - acc: 0.9699 - val_loss: 0.2657 - val_acc: 0.9228\n","Epoch 10/10\n","60000/60000 [==============================] - 8s 140us/step - loss: 0.0689 - acc: 0.9739 - val_loss: 0.2978 - val_acc: 0.9195\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe54a27de80>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lGTA3bfEJjwa"},"source":["### Now, to the above model, lets add Data Augmentation "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"F6gX8n5SJjwb"},"source":["### Import the ImageDataGenrator from keras and fit the training images"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Cbz4uHBuJjwc","colab":{}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","# This will do preprocessing and realtime data augmentation:\n","datagen = ImageDataGenerator(\n","    featurewise_center=False,  # set input mean to 0 over the dataset\n","    samplewise_center=False,  # set each sample mean to 0\n","    featurewise_std_normalization=False,  # divide inputs by std of the dataset\n","    samplewise_std_normalization=False,  # divide each input by its std\n","    zca_whitening=False,  # apply ZCA whitening\n","    rotation_range=50,  # randomly rotate images in the range (degrees, 0 to 180)\n","    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n","    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n","    horizontal_flip=False,  # randomly flip images\n","    vertical_flip=False)  # randomly flip images\n","\n","# Prepare the generator\n","datagen.fit(x_train)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"pl-8dOo7Jjwf"},"source":["#### Showing 5 versions of the first image in training dataset using image datagenerator.flow()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DpI1_McYJjwg","outputId":"20e97e88-c818-4b28-f049-36e252544482","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1560077444676,"user_tz":-330,"elapsed":1300,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["from matplotlib import pyplot as plt\n","gen = datagen.flow(x_train[0:1], batch_size=1)\n","for i in range(1, 6):\n","    plt.subplot(1,5,i)\n","    plt.axis(\"off\")\n","    plt.imshow(gen.next().squeeze(), cmap='gray')\n","    plt.plot()\n","plt.show()"],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXIAAABcCAYAAABz9T77AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 5 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dmPl5yE8Jjwm"},"source":["### Run the above model using fit_generator()"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"44ZnDdJYJjwn","colab":{"base_uri":"https://localhost:8080/","height":496},"outputId":"f22bd8bb-4a75-4537-e37f-dbd964c43cdd","executionInfo":{"status":"ok","timestamp":1560078056400,"user_tz":-330,"elapsed":180177,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["# Define model\n","    model4 = Sequential()\n","    \n","    # 1st Conv Layer\n","    model4.add(Convolution2D(32, 3, 3, input_shape=(28, 28, 1)))\n","    model4.add(Activation('relu'))\n","\n","    # 2nd Conv Layer\n","    model4.add(Convolution2D(32, 3, 3))\n","    model4.add(Activation('relu'))\n","\n","    # Max Pooling\n","    model4.add(MaxPooling2D(pool_size=(2,2)))\n","    \n","    # Dropout\n","    model4.add(Dropout(0.25))\n","    \n","    # Fully Connected Layer\n","    model4.add(Flatten())\n","    model4.add(Dense(128))\n","    model4.add(Activation('relu'))\n","\n","    # Prediction Layer\n","    model4.add(Dense(10))\n","    model4.add(Activation('softmax'))\n","\n","    # Loss and Optimizer\n","    model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    \n","    # Store Training Results\n","    early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n","    callback_list = [early_stopping]\n","\n","    # Train the model4\n","    model4.fit_generator(datagen.flow(x_train, y_train, \n","                         batch_size=32),\n","                         samples_per_epoch=x_train.shape[0],\n","                         nb_epoch=10, \n","                         validation_data=(x_test, y_test), callbacks=callback_list)\n","    "],"execution_count":17,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(28, 28, 1...)`\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n","  \n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=(array([[[..., callbacks=[<keras.ca..., steps_per_epoch=1875, epochs=10)`\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","1875/1875 [==============================] - 18s 10ms/step - loss: 0.8072 - acc: 0.6975 - val_loss: 0.5875 - val_acc: 0.7848\n","Epoch 2/10\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.5888 - acc: 0.7777 - val_loss: 0.5161 - val_acc: 0.8081\n","Epoch 3/10\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.5221 - acc: 0.8060 - val_loss: 0.4494 - val_acc: 0.8370\n","Epoch 4/10\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.4818 - acc: 0.8198 - val_loss: 0.4311 - val_acc: 0.8466\n","Epoch 5/10\n","1875/1875 [==============================] - 18s 10ms/step - loss: 0.4576 - acc: 0.8291 - val_loss: 0.4242 - val_acc: 0.8500\n","Epoch 6/10\n","1875/1875 [==============================] - 18s 10ms/step - loss: 0.4357 - acc: 0.8369 - val_loss: 0.3952 - val_acc: 0.8580\n","Epoch 7/10\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.4188 - acc: 0.8436 - val_loss: 0.3861 - val_acc: 0.8610\n","Epoch 8/10\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.4049 - acc: 0.8497 - val_loss: 0.3619 - val_acc: 0.8723\n","Epoch 9/10\n","1875/1875 [==============================] - 18s 9ms/step - loss: 0.3944 - acc: 0.8538 - val_loss: 0.4090 - val_acc: 0.8564\n","Epoch 10/10\n","1875/1875 [==============================] - 18s 10ms/step - loss: 0.3843 - acc: 0.8571 - val_loss: 0.3483 - val_acc: 0.8710\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7fe54137cc50>"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MwQQW5iOJjwq"},"source":["###  Report the final train and validation accuracy"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"c1SrtBEPJjwq","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"2d5630c8-044c-49bc-9e7f-546eecd01466","executionInfo":{"status":"ok","timestamp":1560078092068,"user_tz":-330,"elapsed":1343,"user":{"displayName":"Raja Dasgupta","photoUrl":"","userId":"15411214172857292313"}}},"source":["model4.evaluate(x_test,y_test)"],"execution_count":20,"outputs":[{"output_type":"stream","text":["10000/10000 [==============================] - 1s 51us/step\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[0.34827739075422287, 0.871]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ZBwVWNQC2qZD","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}